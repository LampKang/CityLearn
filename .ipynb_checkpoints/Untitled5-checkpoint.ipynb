{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ailab6\\.conda\\envs\\citylearn\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from citylearn import  CityLearn\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Select the climate zone and load environment\n",
    "climate_zone = 1\n",
    "data_path = Path(\"data/Climate_Zone_\"+str(climate_zone))\n",
    "building_attributes = data_path / 'building_attributes.json'\n",
    "weather_file = data_path / 'weather_data.csv'\n",
    "solar_profile = data_path / 'solar_generation_1kW.csv'\n",
    "building_state_actions = 'buildings_state_action_space.json'\n",
    "building_ids = [\"Building_1\",\"Building_2\",\"Building_3\",\"Building_4\",\"Building_5\",\"Building_6\",\"Building_7\",\"Building_8\",\"Building_9\"]\n",
    "objective_function = ['ramping','1-load_factor','average_daily_peak','peak_demand','net_electricity_consumption']\n",
    "\n",
    "env = CityLearn(data_path, building_attributes, weather_file, solar_profile, building_ids, buildings_states_actions = building_state_actions, cost_function = objective_function)\n",
    "observations_spaces, actions_spaces = env.get_state_action_spaces()\n",
    "\n",
    "# Provides information on Building type, Climate Zone, Annual DHW demand, Annual Cooling Demand, Annual Electricity Demand, Solar Capacity, and correllations among buildings\n",
    "building_info = env.get_building_information()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gamma = 0.99\n",
    "update_interval = 5\n",
    "actor_lr = 0.0005\n",
    "critic_lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        \n",
    "    def append_sample(self, sample):\n",
    "        self.buffer.append(sample)\n",
    "        \n",
    "    def sample(self, sample_size):\n",
    "        s, a, r, s_next, done = [],[],[],[],[]\n",
    "        \n",
    "        if sample_size > len(self.buffer):\n",
    "            sample_size = len(self.buffer)\n",
    "            \n",
    "        rand_sample = random.sample(self.buffer, sample_size)\n",
    "        for values in rand_sample:\n",
    "            s.append(values[0])\n",
    "            a.append(values[1])\n",
    "            r.append(values[2])\n",
    "            s_next.append(values[3])\n",
    "            done.append([4])\n",
    "        return torch.tensor(s,dtype=torch.float32).cuda(), torch.tensor(a,dtype=torch.float32).cuda(), torch.tensor(r,dtype=torch.float32).cuda(), torch.tensor(s_next,dtype=torch.float32).cuda(), done\n",
    "    \n",
    "    def __len__(self):\n",
    "         return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.std_bound = std_bound\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        dense_1 = Dense(32, activation='relu')(state_input)\n",
    "        dense_2 = Dense(32, activation='relu')(dense_1)\n",
    "        out_mu = Dense(self.action_dim, activation='tanh')(dense_2)\n",
    "        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
    "        std_output = Dense(self.action_dim, activation='softplus')(dense_2)\n",
    "        return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        mu, std = self.model.predict(state)\n",
    "        mu, std = mu[0], std[0]\n",
    "        return np.random.normal(mu, std, size=self.action_dim)\n",
    "\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / \\\n",
    "            var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, mu, std, actions, advantages):\n",
    "        log_policy_pdf = self.log_pdf(mu, std, actions)\n",
    "        loss_policy = log_policy_pdf * advantages\n",
    "        return tf.reduce_sum(-loss_policy)\n",
    "\n",
    "    def train(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.model(states, training=True)\n",
    "            loss = self.compute_loss(mu, std, actions, advantages)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, state_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_Agents_A2C:\n",
    "    def __init__(self, building_info, observation_spaces = None, action_spaces = None):\n",
    "        \n",
    "        #Hyper-parameters\n",
    "        self.discount = 0.992 #Discount factor\n",
    "        self.batch_size = 100 #Size of each MINI-BATCH\n",
    "        self.iterations = 1 # Number of updates of the actor-critic networks every time-step\n",
    "        self.policy_freq = 2 # Number of iterations after which the actor and target networks are updated\n",
    "        self.tau = 5e-3 #Rate at which the target networks are updated\n",
    "        self.lr_init = 1e-3 #5e-2\n",
    "        self.lr_final = 1e-3 #3e-3\n",
    "        self.lr_decay_rate = 1/(78*8760)\n",
    "        self.expl_noise_init = 0.75 # Exploration noise at time-step 0\n",
    "        self.expl_noise_final = 0.01 # Magnitude of the minimum exploration noise\n",
    "        self.expl_noise_decay_rate = 1/(290*8760)  # Decay rate of the exploration noise in 1/h\n",
    "        self.policy_noise = 0.025*0\n",
    "        self.noise_clip = 0.04*0\n",
    "        self.max_action = 0.25\n",
    "        self.min_samples_training = 400 #Min number of tuples that are stored in the batch before the training process begins\n",
    "        \n",
    "        # Parameters\n",
    "        self.device = \"cuda:0\"\n",
    "        self.time_step = 0\n",
    "        self.building_info = building_info # Can be used to create different RL agents based on basic building attributes or climate zones\n",
    "        self.observation_spaces = observation_spaces\n",
    "        self.action_spaces = action_spaces\n",
    "        self.n_buildings = len(observation_spaces)\n",
    "        self.buffer = {i: Buffer() for i in range(self.n_buildings)}\n",
    "        self.networks_initialized = False\n",
    "        \n",
    "        # Monitoring variables (one per agent)\n",
    "        self.actor_loss_list = {i: [] for i in range(self.n_buildings)}\n",
    "        self.critic1_loss_list = {i: [] for i in range(self.n_buildings)}\n",
    "        self.critic2_loss_list = {i: [] for i in range(self.n_buildings)}\n",
    "        self.q_val_list = {i: [] for i in range(self.n_buildings)}\n",
    "        self.q1_list = {i: [] for i in range(self.n_buildings)}\n",
    "        self.q2_list = {i: [] for i in range(self.n_buildings)}\n",
    "        self.a_track1 = []\n",
    "        self.a_track2 = []\n",
    "        \n",
    "        #Networks and optimizers (one per agent)\n",
    "        self.actor, self.critic = {}, {}\n",
    "        for i, (o, a) in enumerate(zip(observation_spaces, action_spaces)):\n",
    "            # A2C\n",
    "            self.state_dim = o.shape[0]\n",
    "            self.action_dim = a.shape[0]\n",
    "            self.action_bound = a.high[0]\n",
    "            self.std_bound = [1e-2, 1.0]\n",
    "            \n",
    "            self.actor[i] = Actor(self.state_dim, self.action_dim, self.action_bound, self.std_bound)\n",
    "            self.critic[i] = Critic(self.state_dim)\n",
    "            \n",
    "    def select_action(self, states):\n",
    "   \n",
    "        actions = []\n",
    "        for i, state in enumerate(states):\n",
    "            a = self.actor[i]\n",
    "            action = a.get_action(state)\n",
    "            a = np.clip(action, -self.action_bound, self.action_bound)\n",
    "            actions.append(a)\n",
    "        return actions\n",
    "    \n",
    "    def add_to_buffer(self, states, actions, rewards, next_states, dones):\n",
    "        # Information contained in the building_info variable can be used to choose the number of buffers and what information goes to each buffer\n",
    "        \n",
    "        dones = [dones for _ in range(self.n_buildings)]\n",
    "        \n",
    "        for i, (s, a, r, s_next, done) in enumerate(zip(states, actions, rewards, next_states, dones)):\n",
    "            s = (s - self.observation_spaces[i].low)/(self.observation_spaces[i].high - self.observation_spaces[i].low + 0.00001)\n",
    "            s_next = (s_next - self.observation_spaces[i].low)/(self.observation_spaces[i].high - self.observation_spaces[i].low + 0.00001)\n",
    "            self.buffer[i].append_sample((s, a, r, s_next, done))\n",
    "\n",
    "        lr = max(self.lr_final, self.lr_init * (1 - self.time_step * self.lr_decay_rate))\n",
    "        for i in range(self.n_buildings):\n",
    "            self.actor_optimizer[i] = optim.Adam(self.actor[i].parameters(), lr=lr)\n",
    "            self.critic_optimizer[i] = optim.Adam(self.critic[i].parameters(), lr=lr)\n",
    "            \n",
    "        #One TD3 control agent for each building\n",
    "        for i in range(self.n_buildings):\n",
    "            \n",
    "            #Learning begins when a minimum number of tuples have beena added to the buffer\n",
    "            if len(self.buffer[i]) > self.min_samples_training:\n",
    "                \n",
    "                #Every time-step we randomly sample 'self.iterations' number of minibatches from the buffer of experiences and perform 'self.iterations' number of updates of the networks.\n",
    "                for k in range(self.iterations):\n",
    "                    state, action, reward, next_state, dones_mask = self.buffer[i].sample(self.batch_size)\n",
    "                    target_Q = reward.unsqueeze(dim=-1)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "                        \n",
    "                        # Select action according to policy\n",
    "                        next_action = (self.actor_target[i](next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "                        \n",
    "                        # Compute the target Q value\n",
    "                        target_Q1, target_Q2 = self.critic_target[i](next_state, next_action)\n",
    "                        target_Q = torch.min(target_Q1, target_Q2)\n",
    "                        target_Q = reward.unsqueeze(dim=-1) + target_Q * self.discount\n",
    "                        \n",
    "                    # Get current Q estimates\n",
    "                    current_Q1, current_Q2 = self.critic[i](state, action)    \n",
    "                    \n",
    "                    # Compute critic loss\n",
    "                    critic1_loss = F.mse_loss(current_Q1, target_Q)\n",
    "                    critic2_loss = F.mse_loss(current_Q2, target_Q)\n",
    "                    critic_loss = critic1_loss + critic2_loss\n",
    "                    \n",
    "                    # Optimize the critic\n",
    "                    self.critic_optimizer[i].zero_grad()\n",
    "                    critic_loss.backward()  \n",
    "                    self.critic_optimizer[i].step()\n",
    "                    \n",
    "                    # Save values\n",
    "                    self.q_val_list[i].append(target_Q)\n",
    "                    self.q1_list[i].append(current_Q1)\n",
    "                    self.q2_list[i].append(current_Q2)\n",
    "                    self.critic1_loss_list[i].append(critic1_loss)\n",
    "                    self.critic2_loss_list[i].append(critic2_loss)\n",
    "                    \n",
    "                    # Delayed policy updates\n",
    "                    if k % self.policy_freq == 0:\n",
    "                        \n",
    "                        # Compute actor loss\n",
    "                        actor_loss = -self.critic[i].Q1(state, self.actor[i](state)).mean()\n",
    "                        self.actor_loss_list[i].append(actor_loss)\n",
    "                                        \n",
    "                        # Optimize the actor\n",
    "                        self.actor_optimizer[i].zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        self.actor_optimizer[i].step()\n",
    "\n",
    "                        # Update the frozen target models\n",
    "                        for param, target_param in zip(self.critic[i].parameters(), self.critic_target[i].parameters()):\n",
    "                            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "                        for param, target_param in zip(self.actor[i].parameters(), self.actor_target[i].parameters()):\n",
    "                            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        self.time_step += 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL CONTROLLER\n",
    "#Instantiating the control agent(s)\n",
    "agents = RL_Agents_A2C(building_info, observations_spaces, actions_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select many episodes for training. In the final run we will set this value to 1 (the buildings run for one year)\n",
    "episodes = 1\n",
    "\n",
    "k, c = 0, 0\n",
    "cost, cum_reward = {}, {}\n",
    "\n",
    "# The number of episodes can be replaces by a stopping criterion (i.e. convergence of the average reward)\n",
    "for e in range(episodes):     \n",
    "    cum_reward[e] = 0\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        if k%(1000)==0:\n",
    "            print('hour: '+str(k)+' of '+str(8760*episodes))\n",
    "            \n",
    "        action = agents.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agents.add_to_buffer(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        cum_reward[e] += reward[0]\n",
    "        rewards.append(reward)\n",
    "        k+=1\n",
    "        \n",
    "    cost[e] = env.cost()\n",
    "    if c%20==0:\n",
    "        print(cost[e])\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "      dtype=float32),\n",
       "       array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  2.36,  0.  ,\n",
       "        0.  ], dtype=float32),\n",
       "       array([ 1.  , 17.81, 25.29,  1.65,  0.  ], dtype=float32),\n",
       "       array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  0.46,  0.  ],\n",
       "      dtype=float32),\n",
       "       array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "      dtype=float32),\n",
       "       array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "      dtype=float32),\n",
       "       array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "      dtype=float32),\n",
       "       array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "      dtype=float32),\n",
       "       array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "      dtype=float32)], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  2.36,  0.  ,\n",
       "        0.  ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1fad23fdbdfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'states' is not defined"
     ]
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d2379a17688b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mactions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'actions' is not defined"
     ]
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "action=agents.select_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([array([ 2.        , 16.14      , 25.96      ,  0.        , 41.67      ,\n",
       "         0.        , 65.46      ,  0.23965307,  0.33333333]),\n",
       "        array([ 2.  , 16.14, 25.96,  0.  , 41.67,  0.  , 65.46,  1.91,  0.  ,\n",
       "         0.  ]),\n",
       "        array([ 2.  , 16.14, 25.96,  1.59,  0.  ]),\n",
       "        array([ 2.        , 16.14      , 25.96      ,  0.        , 41.67      ,\n",
       "         0.        , 65.46      ,  0.42      ,  0.33333334]),\n",
       "        array([ 2.        , 16.14      , 25.96      ,  0.        , 41.67      ,\n",
       "         0.        , 65.46      ,  0.26345017,  0.        ]),\n",
       "        array([ 2.        , 16.14      , 25.96      ,  0.        , 41.67      ,\n",
       "         0.        , 65.46      ,  0.        ,  0.32270916]),\n",
       "        array([ 2.        , 16.14      , 25.96      ,  0.        , 41.67      ,\n",
       "         0.        , 65.46      ,  0.        ,  0.10690593]),\n",
       "        array([ 2.        , 16.14      , 25.96      ,  0.        , 41.67      ,\n",
       "         0.        , 65.46      ,  0.28464492,  0.31884058]),\n",
       "        array([ 2.        , 16.14      , 25.96      ,  0.        , 41.67      ,\n",
       "         0.        , 65.46      ,  0.33333334,  0.        ])], dtype=object),\n",
       " [-151.57613987063027,\n",
       "  -6.534293740242953,\n",
       "  -4.568468081102065,\n",
       "  -104.95494331891655,\n",
       "  -52.80387645981315,\n",
       "  -142.5926563454005,\n",
       "  -76.87148300591961,\n",
       "  -156.59025762869118,\n",
       "  -70.11580103682742],\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ailab6\\.conda\\envs\\citylearn\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'max'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-418ff217668b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\CityLearn - 복사본\\citylearn.py\u001b[0m in \u001b[0;36mcost\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;31m# Average of all the daily peaks of the 365 day of the year. The peaks are calculated using the net energy demand of the whole district of buildings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'average_daily_peak'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mcost\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'average_daily_peak'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_electric_consumption\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_electric_consumption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_rbc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'average_daily_peak'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m# Peak demand of the district for the whole year period.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\CityLearn - 복사본\\citylearn.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;31m# Average of all the daily peaks of the 365 day of the year. The peaks are calculated using the net energy demand of the whole district of buildings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'average_daily_peak'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mcost\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'average_daily_peak'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_electric_consumption\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_electric_consumption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_rbc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'average_daily_peak'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m# Peak demand of the district for the whole year period.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'max'"
     ]
    }
   ],
   "source": [
    "env.cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.  , 17.81, 25.29,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = observations_spaces[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-citylearn]",
   "language": "python",
   "name": "conda-env-.conda-citylearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
